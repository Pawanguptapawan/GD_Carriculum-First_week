{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d59620-3955-45c1-a38b-27ff40c8dceb",
   "metadata": {},
   "source": [
    "# what is finetunning:  taking these general purpose models  and turning into  specailized models.\n",
    "# reduce hallucinatons\n",
    "# customize the model to a specific use case\n",
    "\n",
    "# pros: data nearly unlimited data fits  , learn new information ,  correct incorrect info ,  less cost afterwards it samlled model , use RAG too,\n",
    "# cos : more high quality data,upfront compute cost,needs some technical knowledge.\n",
    "\n",
    "\n",
    "# Benefits of finetunnig LLM: performance , stop hallucinations, reduce unwanted info, privacy , prevent leakage, no breaches,cost, lower cost per request, increased transparency,reliability , control uptime,lower latency,moderations , \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b933af8-e689-4854-b758-8ac62e23faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lamini import Lamini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce40bc1c-89a7-4708-a592-881148a51d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama import BasicModelRunner\n",
    "no_finetunned=BasicModelRunner(\"meta-llama/Llama-2-7b-hf\")\n",
    "non_finetuned_output=non_finetunned(\"What is your name?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f12eafe-9cea-4d46-a6f6-070652935a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model=BasicModelRunner(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "finetuned_output=finetuned_model(\"What is your name?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9969031-ddfb-4e56-a5af-5f08dd9f9cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(non_finetuned(\"[INST] tell me how to train my dog?\"))\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da2d420-9ae4-49df-904d-38f0259c80f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where finetuning fit in : pretraining -> finetuning ->  again finetuning can be done.\n",
    "# finetuning usually refers to training further can also be self-supervised unlabeled data,can be labeld data you curated, much less data needed.\n",
    "# finetuning for generative tasks is not well defined: update entire model , not just part of it, smae training objetive : next token prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d8783-7a70-4132-b5f9-ea20bd128434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is finetuning doing for us:\n",
    "# behave change : learning to respond more consitently\n",
    "# learning ro focus\n",
    "# gain knowledge: increasing knowledge of new specific concepts,\n",
    "# correcting old incorrect info.\n",
    "# both : \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27106b3d-bd1b-4bb0-b54a-170bfd4b5033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tasks to finetune\n",
    "# just test-in, text-out:\n",
    "    # Extraction : text in , less text out\n",
    "        # reading \n",
    "        # keywords, topics, routing, agents\n",
    "    #expansion:text in, more text out\n",
    "        # writing \n",
    "        # chat , write emails\n",
    "# tasks clarity is key indicator of success\n",
    "# clarity means knowing what's bad and good and better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09029cf6-a2fd-4a2b-a0c8-428fa7104e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first time finetunning\n",
    "# 1. identify task by prompt engi. a LLM\n",
    "# 2. find task that you see on LLM doing ~ok at.\n",
    "# 3.pick one task.\n",
    "# 4. get ~1000 inputs and outputs for the task.\n",
    "# 5.finetune a small LLM on this data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99e821ee-8204-4cdc-bb99-ea1c69c7b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines \n",
    "import itertools\n",
    "import pandas as pd\n",
    "import pprint import pprint\n",
    "import datasets\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29042527-7977-49e9-a3cc-82841c0f1167",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_dataset=load_dataset('EleutherAI/pile',split='train',streaming=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6f6571-7ba0-4652-a587-a7a20110cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=5\n",
    "top_n=itertools.islice(pretrained_dataset,n)\n",
    "for i in top_n:\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15deba-0434-4a16-894d-cd6b9b0f6253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps to prepare the data.\n",
    "# @ collect instruction - response pairs\n",
    "# tokenize : pad,truncate\n",
    "# split into train/text\n",
    "# tokenize the data: convert the words  into numbers\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"\")\n",
    "text='Hi , I am Pawan'\n",
    "encoded_text=tokenizer(text)['input_ids']\n",
    "print(encoded_text)\n",
    "decoded_text=tokenizer.decode(endoded_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9131c9e3-037b-4a22-817f-0f5f62a8305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_texts=['hi', How are you?','I am good','yes']\n",
    "# padding\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "encoded_texts_longest=tokenizer(list_texts,padding=True)\n",
    "print(encoded_texts_longest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ef5ffe3-51e2-47f8-9de5-0590d881cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncation\n",
    "encoded_texts_truncation=tokenizer(list_texts,max_length=3,truncation=True)\n",
    "print(encoded_text_truncation['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb316e3f-da6f-43a9-98bd-f0dc157a0957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and padding\n",
    "encoded_texts_truncation=tokenizer(list_texts,max_length=3,truncation=True,padding=True)\n",
    "print(encoded_text_truncation['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8ce7e4f-d037-4964-acd1-6712cb467551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    if 'question' in examples and 'answer' in examples:\n",
    "        text=examples['question'][0]+examples['answers'][0]\n",
    "    elif 'input' in examples and 'output' in examples:\n",
    "        text=examples['input']+examples['output'][0]\n",
    "    else:\n",
    "        text=examples['text'][0]\n",
    "\n",
    "    tokenizer.pad_token=tokenizer.eos_token\n",
    "    tokenized_input=tokenizer(text,return_tensor='np',padding=True)\n",
    "    max_length=min(tokenized_inputs['input_ids'].shape[1],2048)\n",
    "    tokenizer.truncation_side='left'\n",
    "    tokenized_inputs=tokenizer(text,return_tensors='np',truncation=True,max_length=max_length)\n",
    "    return tokenized_inputs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cdc8c4-7ad2-4a88-a9a2-b060fb59dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetunnig_dataset_loaded=datasets.load_dataset('json',data_files=filename)\n",
    "tokenized_dataset=finetuning_dataset_loaded.map(tokenize_function,batched=True,batch_size=1,drop_last_batch=True)\n",
    "print(tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190769f5-b4dd-4d32-9bfd-9038f49ca746",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset=tokenized_dataset.ad_column('labels',tokenized_dataset['input_ids'])\n",
    "split_dataset=tokenized_dataset.train_test_split(text_sie=0.1,shuffle=True,seed=123)\n",
    "print(split_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b87352d-25cd-47e5-ab4b-16bd40e800f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training Process\n",
    "\n",
    "# add training data\n",
    "# calculate loss\n",
    "# backprop through model\n",
    "# update weights\n",
    "# st hyperparameters like: learning rate,learning rate scheduler , optimize hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84806d7d-a170-4a53-9b2e-97e680f91a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "#     for batch in train_dataloades:\n",
    "#         outputs=model(**batch)\n",
    "#         loss=outputs.loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82209695-3ee5-4e5d-8fb1-737bf6644351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from utilities import *\n",
    "from transformers import AutoTokenizer,AutoModelForCasualLM,AutoModelForCasualLm\n",
    "from llama import BasicModelRunner\n",
    "from llama import BasicModelRunner\n",
    "\n",
    "logger=logging.getLogger(__name__)\n",
    "global_config=None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fcf08-bbcb-4e39-b4fd-1cc2cc8574ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name='lamini_docs.jsonl'\n",
    "dataset_path=''\n",
    "use_hf=False\n",
    "dataset_path=''\n",
    "use_hf=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd437466-126b-4680-bf6a-3649a5ad09a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=''\n",
    "training_config={\n",
    "    'model':{\n",
    "        'pretrained_name':model_name,\n",
    "        'max_length':2048,\n",
    "    },\n",
    "    'dataset':{\n",
    "        'use_hf':use_hf,\n",
    "        'path':dataset_path\n",
    "    },\n",
    "    'verbose':True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378ad3e1-d449-43fd-99c6-5d6520dce443",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"\")\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "train_dataset,text_dataset=tokenize_and_split_data(training_config,tokenizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b38dd5c-099f-4128-8ff2-9a0ca6935b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoModelFoerCasualLM.from_pretrained(model_name)\n",
    "\n",
    "device_count=torch.cuda.device_count\n",
    "if device_count>0:\n",
    "    logger.debug(\"select gpu device\")\n",
    "    device=torch.device(\"cude\")\n",
    "else:\n",
    "    logger.debug(\"Select CPU device\")\n",
    "    device=torch.device(\"CPU\")\n",
    "\n",
    "base_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8d37e2-0366-45b2-acb8-dee51a440f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text,model,tokenize,max_input_tokens=1000,max_output_token=100):\n",
    "    input_ids=tokenizer.encode(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        max_length=max_input_tokens\n",
    "    )\n",
    "\n",
    "    device=model.device\n",
    "    generated_token_with_prompt=model.generate(input_ids=input_ids.to(device),max_length=max_output_tokens)\n",
    "    generated_text_with_prompt=tokenizer.batch_decode(generate_tokens_with_prompt,skip_special_token=True)\n",
    "    generated_text_answer=generated_text_with_prompt[0][len(text):]\n",
    "    return  generated_text_answer\n",
    "\n",
    "\n",
    "max_steps=3\n",
    "trained_model_name=f'{max_steps}'\n",
    "output_dir=trained_model_name\n",
    "\n",
    "training_args=TrainingArguments(\n",
    "    learning_rate=1.0e-5,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=max_steps,\n",
    "    per_device_train_batch_size=1,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=False,\n",
    "    disable_tqdm=False,\n",
    "    eval_steps=120,\n",
    "    save_steps=120,\n",
    "    warmup_steps=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy='steps',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=1,\n",
    "    optim='adafactor',\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=False,\n",
    "    logging_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca646cd-19ff-4f3f-9805-9d253e9d715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_flops=(\n",
    "    base_model.floating_point_ops(\n",
    "        {\n",
    "            'input_ids':torch.zeros(\n",
    "                (1,training_config['mode']['max_length'])\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    *training_args.gradient_accumulation_steps\n",
    ")\n",
    "print(base_model)\n",
    "print(base_mode.get_memory_footprint()/1e9)\n",
    "print(model_flops/1e9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6bb593-20cb-4887-b955-da00fcac4451",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=Trainer(\n",
    "    model=base_model,\n",
    "    model_flops=model_flops,\n",
    "    total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "training_output=trainer.train()\n",
    "\n",
    "save_dir=f'{output_dir}/final'\n",
    "trainer.save_model(save_dir)\n",
    "\n",
    "finetuned_slightly_model=AutoodelForCasualLM.from_pretrained(save_dir,local_files_only=True)\n",
    "\n",
    "\n",
    "finetuned_slightly_model.to(device)\n",
    "\n",
    "text_question=test_dataset[0]['question']\n",
    "print(text_question)\n",
    "print(inference(text_question,finetuned_slightly_modee,tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa549ca2-9b42-4037-90d9-1d4ff0587f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answer=test_dataset[0]['answer']\n",
    "print(test_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c5064-8db8-4908-bd38-28b841e56b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_longer_model=AutoModelForCasualLM.from_pretrained('')\n",
    "toeknizer=AutoTokenizer.from_pretrained('')\n",
    "finetuned_longer_model.to(device)\n",
    "print(inference(test_question,finetuned_longer_model,tokenizer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a661dc-7196-4918-b77e-4e386dcce368",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_finetuned_model=BasicModelRunner(model_name_to_id['bigger_model_name'])\n",
    "bigger_finetuned_output=bigger_finetuned_model(test_question)\n",
    "print(bigger_finetuned_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7120cc6-7fd2-4f3c-a1f6-e0ed48d9ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for i in range(len(train_dataset)):\n",
    "    if 'keep the discussion relevant to Lamini' in train_dataset[i]['answer']:\n",
    "        print(i,train_dataset[i]['question'],train_dataset[i]['answer'])\n",
    "        count+=1\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50d078f-adf4-437a-8b27-3313c799d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=AutoModelForCasualLM.from_pretrained('')\n",
    "base_tokenizer=AutoTokenizer.from_pretrained('')\n",
    "print(inference('what do you think of mars',base_model,base_tokenizer))\n",
    "\n",
    "\n",
    "print(inference('what do you think of mars',finetuned_longer_model,tokenizer))\n",
    "\n",
    "print(inference('what do you think of mars',finetuned_longer_model,tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee13bc3d-986b-4b93-a94a-aefbe3abe1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=BasicModelRunner('')\n",
    "model.load_data_from_jsonlines('')\n",
    "model.train(is_public=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b06659-6d3a-4257-b4d1-77ce2b262236",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e819af-97a6-45cc-a4ab-8359f48384c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lofd=[]\n",
    "for e in out['eval_results']:\n",
    "    q=f'{e['input']}'\n",
    "    at=f'{e['outputs'][0]['output']}'\n",
    "    ab=f'{e['outputs'][0]1['output']}'\n",
    "    di={'question':q,'trained_model':at,'base_mode':ab}\n",
    "    lofd.append(di)\n",
    "\n",
    "df=pd.DataFrame.from_dict(lofd)\n",
    "style_df=df.style.set_properties(**{'text-align':'left'})\n",
    "style_df=style_df.style.set_properties(**{'vertical-align':'text-top'})\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5865f7-e81d-488b-8e26-bcaa8a570b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation and iteration\n",
    "dataset=datasets.load_dataset('')\n",
    "test_dataset=dataset['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ced94-aaf5-41f3-87ef-1c9e7359e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=''\n",
    "model=AutoModelForCasualLM.from_pretrained('')\n",
    "toeknizer=AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def is_exact_match(a,b):\n",
    "    return a.strip()==b.strip()\n",
    "\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c1c15-a3e1-4b1f-9812-6749dc437ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_question=text_dataset[0]['question']\n",
    "generated_answer=inference(text_question,model,tokenizer)\n",
    "print(is_exact_match(generated<answer,answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf522f70-c29b-4049-bea9-c8ca42af61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10\n",
    "metrics={'exact_matches':[]}\n",
    "predictions=[]\n",
    "for i,item in tqdm(enumerated(test_dataset)):\n",
    "    question=item['question']\n",
    "    answer=item['answer']\n",
    "\n",
    "    try:\n",
    "        predicted_answer=inference(question,model,tokenizer)\n",
    "    except:\n",
    "        continue\n",
    "    predictions.append([predicted_answer,answer])\n",
    "    exact_match=is_excat_match(generated_answer,answer)\n",
    "    metrics['exact_matches'].append(exact_match)\n",
    "\n",
    "    if i>n and n!=-1:\n",
    "        break\n",
    "\n",
    "print(sum(metrics['exact_matches']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397de7e8-86d5-4d49-a75c-b42d0244c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(predictions,columns=['predicted_answer','target_answer'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d3d94-ec9c-43c9-97e3-ae9d2f171134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prectical approach\n",
    "# 1. figure task\n",
    "# 2.collect data related to the task's input/output\n",
    "# 3.generate data if you don't have enough data\n",
    "# 4.finetune a small model\n",
    "# 5.vary the amount of data you give the model\n",
    "# 6. evaluate your LLM to know what's going well vs not\n",
    "# 7. Collect more data to improve\n",
    "# 8. Increase task complexity\n",
    "# peft=parameter - efficient finetunning\n",
    "# lora=low rank adaption of LLMs:\n",
    "    # fewer trainble parameters: for GPT 3, 100000x less\n",
    "    # less gpu memory: for GPT3 , 3x less\n",
    "    # slightly below accuracy to finetuning\n",
    "    # same inference latency\n",
    "# train new weights in some layers,freeze main weights\n",
    "    # new weights,rank decomposition matrices of original weights change\n",
    "    # at inference, merge with main weights\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
